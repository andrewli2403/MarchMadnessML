{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in processed data from CollegeBasketballPredictor.ipynb\n",
    "def load_marchmadness():\n",
    "    #features from training, test, validation sets\n",
    "    featurestrainingData = torch.load('featurestraining_pytorch.pt')\n",
    "    featurestestData = torch.load('featurestest_pytorch.pt')\n",
    "    featuresvalidationData = torch.load('featuresvalidation_pytorch.pt')\n",
    "    \n",
    "    #labels from training, test, validation sets\n",
    "    labelstrainingData = torch.load('labelstraining_pytorch.pt')\n",
    "    labelstestData = torch.load('labelstest_pytorch.pt')\n",
    "    labelsvalidationData = torch.load('labelsvalidation_pytorch.pt')\n",
    "    \n",
    "    return featurestrainingData, labelstrainingData, featurestestData, labelstestData, featuresvalidationData, labelsvalidationData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim, output_dim):\n",
    "    # We don't need the softmax layer here since CrossEntropyLoss already\n",
    "    # uses it internally.\n",
    "    model = torch.nn.Sequential()\n",
    "    model.add_module(\"linear\",\n",
    "                     torch.nn.Linear(input_dim, output_dim, bias=False))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss, optimizer, x, y):\n",
    "    # Reset gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward\n",
    "    fx = model.forward(x)\n",
    "    output = loss.forward(fx, y)\n",
    "\n",
    "    # Backward\n",
    "    output.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    return output.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "    output = model.forward(x)\n",
    "    return output.data.numpy().argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    torch.manual_seed(42)\n",
    "    #X is data Y is label\n",
    "    trX, trY, teX, teY, valX, valY = load_marchmadness()\n",
    "    \n",
    "    n_examples, n_features = list(trX.size())[0], list(trX.size())[1]\n",
    "    n_classes = 2\n",
    "    model = build_model(n_features, n_classes)\n",
    "    loss = torch.nn.CrossEntropyLoss(reduction='elementwise_mean')\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    batch_size = 100\n",
    "\n",
    "    for i in range(100):\n",
    "        cost = 0.\n",
    "        num_batches = n_examples // batch_size\n",
    "        for k in range(num_batches):\n",
    "            start, end = k * batch_size, (k + 1) * batch_size\n",
    "            cost += train(model, loss, optimizer,\n",
    "                          trX[start:end], trY[start:end])\n",
    "        predY = predict(model, teX)\n",
    "        print(\"Epoch %d, cost = %f, acc = %.2f%%\"\n",
    "              #.cpu().numpy() turns it back into a numpy array\n",
    "              % (i + 1, cost / num_batches, 100. * np.mean(predY == teY.cpu().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:16: UserWarning: reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(\"reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, cost = 18680.338339, acc = 49.39%\n",
      "Epoch 2, cost = 15630.104081, acc = 63.61%\n",
      "Epoch 3, cost = 15270.767256, acc = 50.61%\n",
      "Epoch 4, cost = 11571.682060, acc = 70.85%\n",
      "Epoch 5, cost = 14266.887099, acc = 50.79%\n",
      "Epoch 6, cost = 3834.958311, acc = 66.41%\n",
      "Epoch 7, cost = 1970.878921, acc = 49.54%\n",
      "Epoch 8, cost = 819.781875, acc = 82.79%\n",
      "Epoch 9, cost = 671.177388, acc = 81.71%\n",
      "Epoch 10, cost = 701.495142, acc = 65.75%\n",
      "Epoch 11, cost = 729.243179, acc = 90.00%\n",
      "Epoch 12, cost = 550.017137, acc = 76.79%\n",
      "Epoch 13, cost = 539.084890, acc = 58.46%\n",
      "Epoch 14, cost = 678.705344, acc = 49.58%\n",
      "Epoch 15, cost = 528.027367, acc = 75.04%\n",
      "Epoch 16, cost = 428.361453, acc = 71.92%\n",
      "Epoch 17, cost = 614.781588, acc = 62.80%\n",
      "Epoch 18, cost = 473.621343, acc = 92.76%\n",
      "Epoch 19, cost = 803.190519, acc = 49.39%\n",
      "Epoch 20, cost = 2312.080110, acc = 86.66%\n",
      "Epoch 21, cost = 353.466532, acc = 93.46%\n",
      "Epoch 22, cost = 280.150261, acc = 90.88%\n",
      "Epoch 23, cost = 247.669779, acc = 93.02%\n",
      "Epoch 24, cost = 259.891731, acc = 93.57%\n",
      "Epoch 25, cost = 1227.973464, acc = 93.52%\n",
      "Epoch 26, cost = 350.481635, acc = 49.47%\n",
      "Epoch 27, cost = 2548.503558, acc = 93.79%\n",
      "Epoch 28, cost = 184.809898, acc = 93.99%\n",
      "Epoch 29, cost = 232.288911, acc = 92.74%\n",
      "Epoch 30, cost = 205.856132, acc = 78.26%\n",
      "Epoch 31, cost = 333.746416, acc = 86.07%\n",
      "Epoch 32, cost = 232.501392, acc = 94.33%\n",
      "Epoch 33, cost = 249.223471, acc = 77.75%\n",
      "Epoch 34, cost = 336.195788, acc = 66.56%\n",
      "Epoch 35, cost = 204.347383, acc = 89.13%\n",
      "Epoch 36, cost = 166.783486, acc = 94.31%\n",
      "Epoch 37, cost = 205.755365, acc = 92.45%\n",
      "Epoch 38, cost = 187.896189, acc = 85.37%\n",
      "Epoch 39, cost = 225.739118, acc = 77.73%\n",
      "Epoch 40, cost = 197.618556, acc = 94.84%\n",
      "Epoch 41, cost = 269.791775, acc = 77.12%\n",
      "Epoch 42, cost = 183.024492, acc = 74.67%\n",
      "Epoch 43, cost = 216.996526, acc = 85.81%\n",
      "Epoch 44, cost = 208.257375, acc = 86.90%\n",
      "Epoch 45, cost = 213.048886, acc = 94.62%\n",
      "Epoch 46, cost = 193.997753, acc = 74.94%\n",
      "Epoch 47, cost = 192.350714, acc = 93.81%\n",
      "Epoch 48, cost = 189.649138, acc = 68.11%\n",
      "Epoch 49, cost = 202.305567, acc = 70.93%\n",
      "Epoch 50, cost = 244.053108, acc = 94.57%\n",
      "Epoch 51, cost = 144.531603, acc = 80.93%\n",
      "Epoch 52, cost = 158.070764, acc = 92.43%\n",
      "Epoch 53, cost = 149.891198, acc = 94.84%\n",
      "Epoch 54, cost = 182.843908, acc = 81.67%\n",
      "Epoch 55, cost = 200.030578, acc = 65.20%\n",
      "Epoch 56, cost = 199.763011, acc = 71.92%\n",
      "Epoch 57, cost = 189.823497, acc = 92.74%\n",
      "Epoch 58, cost = 141.229142, acc = 95.62%\n",
      "Epoch 59, cost = 138.103730, acc = 88.98%\n",
      "Epoch 60, cost = 153.082558, acc = 74.45%\n",
      "Epoch 61, cost = 172.303188, acc = 95.56%\n",
      "Epoch 62, cost = 111.295863, acc = 89.08%\n",
      "Epoch 63, cost = 142.791630, acc = 77.98%\n",
      "Epoch 64, cost = 178.727656, acc = 68.59%\n",
      "Epoch 65, cost = 228.855520, acc = 90.18%\n",
      "Epoch 66, cost = 120.518731, acc = 93.72%\n",
      "Epoch 67, cost = 131.362100, acc = 93.00%\n",
      "Epoch 68, cost = 189.516380, acc = 85.83%\n",
      "Epoch 69, cost = 168.206230, acc = 95.23%\n",
      "Epoch 70, cost = 275.982294, acc = 84.60%\n",
      "Epoch 71, cost = 197.947384, acc = 92.13%\n",
      "Epoch 72, cost = 101.769090, acc = 96.02%\n",
      "Epoch 73, cost = 150.537660, acc = 95.91%\n",
      "Epoch 74, cost = 165.398762, acc = 95.30%\n",
      "Epoch 75, cost = 169.264225, acc = 80.55%\n",
      "Epoch 76, cost = 142.160258, acc = 92.13%\n",
      "Epoch 77, cost = 137.149016, acc = 90.95%\n",
      "Epoch 78, cost = 137.687204, acc = 95.52%\n",
      "Epoch 79, cost = 100.463732, acc = 90.64%\n",
      "Epoch 80, cost = 142.459068, acc = 92.82%\n",
      "Epoch 81, cost = 140.989213, acc = 94.55%\n",
      "Epoch 82, cost = 126.756348, acc = 89.55%\n",
      "Epoch 83, cost = 84.276113, acc = 88.84%\n",
      "Epoch 84, cost = 176.453646, acc = 82.31%\n",
      "Epoch 85, cost = 158.000620, acc = 89.70%\n",
      "Epoch 86, cost = 92.582509, acc = 91.19%\n",
      "Epoch 87, cost = 103.147406, acc = 96.22%\n",
      "Epoch 88, cost = 187.117504, acc = 93.61%\n",
      "Epoch 89, cost = 244.281203, acc = 83.05%\n",
      "Epoch 90, cost = 181.733012, acc = 91.78%\n",
      "Epoch 91, cost = 99.061828, acc = 95.58%\n",
      "Epoch 92, cost = 93.293631, acc = 95.30%\n",
      "Epoch 93, cost = 83.055999, acc = 94.92%\n",
      "Epoch 94, cost = 127.564093, acc = 90.95%\n",
      "Epoch 95, cost = 117.583149, acc = 93.63%\n",
      "Epoch 96, cost = 136.276733, acc = 83.95%\n",
      "Epoch 97, cost = 190.599397, acc = 85.80%\n",
      "Epoch 98, cost = 120.385679, acc = 91.86%\n",
      "Epoch 99, cost = 121.189820, acc = 87.86%\n",
      "Epoch 100, cost = 125.513871, acc = 94.53%\n",
      "Epoch 101, cost = 83.426302, acc = 95.93%\n",
      "Epoch 102, cost = 86.255336, acc = 88.91%\n",
      "Epoch 103, cost = 75.738293, acc = 91.10%\n",
      "Epoch 104, cost = 142.808492, acc = 88.58%\n",
      "Epoch 105, cost = 70.635447, acc = 90.38%\n",
      "Epoch 106, cost = 145.692585, acc = 89.54%\n",
      "Epoch 107, cost = 103.753898, acc = 89.79%\n",
      "Epoch 108, cost = 89.548396, acc = 88.89%\n",
      "Epoch 109, cost = 79.672020, acc = 92.93%\n",
      "Epoch 110, cost = 142.613442, acc = 96.52%\n",
      "Epoch 111, cost = 65.840326, acc = 95.47%\n",
      "Epoch 112, cost = 110.026406, acc = 82.35%\n",
      "Epoch 113, cost = 179.947734, acc = 95.56%\n",
      "Epoch 114, cost = 120.817877, acc = 89.92%\n",
      "Epoch 115, cost = 128.724872, acc = 90.73%\n",
      "Epoch 116, cost = 89.951782, acc = 93.31%\n",
      "Epoch 117, cost = 117.742374, acc = 95.91%\n",
      "Epoch 118, cost = 81.881014, acc = 96.63%\n",
      "Epoch 119, cost = 72.137849, acc = 92.58%\n",
      "Epoch 120, cost = 61.465246, acc = 96.79%\n",
      "Epoch 121, cost = 113.572117, acc = 83.40%\n",
      "Epoch 122, cost = 129.388449, acc = 95.25%\n",
      "Epoch 123, cost = 78.797617, acc = 95.32%\n",
      "Epoch 124, cost = 145.006857, acc = 83.92%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-36dada65edb3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             cost += train(model, loss, optimizer,\n\u001b[0;32m---> 19\u001b[0;31m                           trX[start:end], trY[start:end])\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mpredY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         print(\"Epoch %d, cost = %f, acc = %.2f%%\"\n",
      "\u001b[0;32m<ipython-input-4-0192b7294017>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss, optimizer, x, y)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 904\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1968\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1970\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1293\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m         \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
